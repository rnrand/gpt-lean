<!doctype html>
<html lang="en">
<head>
	<meta charset="utf-8">
    <meta name="robots" content="noindex">
    
	<title>CMSC 27100 &mdash; Lecture 20</title>
	<link rel="stylesheet" type="text/css" href="main.css">
	<link type="../text/css" rel="stylesheet" href="chrome-extension://cpngackimfmofbokmjmljamhdncknpmg/style.css">	

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>
</head>


<body>
	<h1><a href="../">CMSC 27100</a> &mdash; Lecture 20</h1>

    <h2>Bayes' Theorem</h2>

    <p>In the last class, we defined the conditional probability of $A$ given $B$ as $\Pr(A \mid B) = \frac{\Pr(A \cap B)}{\Pr(B)}$ for non-empty $B$. By rearranging the equation slightly, we get $\Pr(A \cap B) = \Pr(A \mid B) \cdot \Pr(B)$, which allows us to make the following observation, named after the Presbyterian minister and statistician Thomas Bayes.

    <p><strong>Theorem 19.1 (Bayes' Theorem).</strong> If $A$ and $B$ are events with $\Pr(A), \Pr(B) \gt 0$, then
    $$\Pr(A \mid B) = \frac{\Pr(B \mid A) \cdot \Pr(A)}{\Pr(B)}$$

    <p><em>Proof.</em> We know that 
		$$\Pr(A \mid B) \Pr(B) = \Pr(A \cap B) = \Pr(B \mid A) \Pr(A)$$
		Dividing all the terms by $\Pr(B)$ get us the desired result.
    $\tag*{$\Box$}$

    <p><strong>Definition 19.2.</strong> A <em>partition</em> of $\Omega$ is a family of pairwise disjoint events $H_1, \dots, H_m$ covering $\Omega$. That is, $\bigcup_{i=1}^m H_i = \Omega$ and $H_i \cap H_j = \emptyset$ for all $i \neq j$.

    <p><strong>Theorem 19.3 (Law of Total Probability).</strong> Given a partition $H_1, \dots, H_m$ of $\Omega$ and an event $A$,
    $$\Pr(A) = \sum_{i=1}^m \Pr(A \cap H_i).$$

    <p><em>Proof.</em> Since we have a partition $H_1, \dots, H_m$, we have $A = A \cap \Omega = A \cap \bigcup_{i = 1}^m H_i$. Then,
    \begin{align*}
    \Pr(A) &= \Pr\left( A \cap \bigcup_{i=1}^m H_i \right) \\
    &= \Pr \left( \bigcup_{i=1}^m (A \cap H_i) \right) \\
    &= \sum_{i=1}^m \Pr(A \cap H_i) &\text{since $H_i$ are pairwise disjoint} \\
    \end{align*}
    $\tag*{$\Box$}$
	
	This can be rewritten as
	$$\Pr(A) = \sum_{i=1}^m \Pr(A \mid H_i) \cdot \Pr(H_i).$$

    <p>The Law of Total Probability generalizes the idea that if I know $\Pr(A \cap B)$ and I know $\Pr(A \cap \overline B)$, then that just gives me $\Pr(A)$ when I put them together. The Law of Total Probability will be particularly useful for computing the probabilities of events when applying Bayes' Theorem.
	
	<p><strong>Example 19.4.</strong> Consider a medical device used to test people for a kind of cancer. This kind of cancer occurs in 1% of the population, that is, $\Pr(C) = 0.01$. The test is relatively accurate, the chance that somebody with cancer tests postive $\Pr(P \mid C)$ is $0.9$ (called the true positive rate or sensitivity) and the probability that somebody without cancer tests positive $\Pr(P \mid \neg C)$ is $0.09$ (the false positive rate). If a random person tests positive, what is the probability they have cancer?
		
	<p>We want to know $\Pr(C \mid P)$. Using Bayes' Theorem, we can write this as 
	$$\Pr(C \mid P) = \frac{\Pr(P \mid C)\Pr(C)}{\Pr(P)} = \frac{0.9 \cdot 0.01}{\Pr(P)}$$
	
	<p>By the law of total probability, 
		$$\Pr(P) = \Pr(P \mid C)\Pr(C) + \Pr(P \mid \neg C)\Pr(\neg C) = 0.9 \cdot 0.01 + 0.09 \cdot 0.99$$
		Hence
	$$\Pr(C \mid P) = \frac{0.9 \cdot 0.01}{0.9 \cdot 0.01 + 0.09 \cdot 0.99} \approx \frac{0.009}{0.009 + 0.09} = \frac{1}{11}$$
    $\tag*{$\Box$}$
	
    <p><strong>Example (Monty Hall) 19.5.</strong> The Monty Hall problem is a classic example of conditional probability and independence and it initially stumpe a remarkable number of serious mathematicians. The story is loosely based on the game show Let's Make a Deal, whose host was a guy by the name of Monty Hall.

    <p>Suppose there are three doors and there is a prize behind each door. Behind one of the doors is a car and behind the other two are goats. So you pick a door. The host will then choose a door to open that reveals a goat. You are then given the option to either stick with your original choice or to change your choice to the remaining closed door. Should you switch?

    <p>There two possible intuitions for this problem. The first is that the goat was equally likely to be behind any one of the three doors. Now that one door has been removed from the equation, the goat is equally likely to be behind each door, for a probability of $\frac{1}{2}$.
		
	<p>The other intuition is that we had an initial probability of $\frac{2}{3}$ of getting a goat. Given that Monty will always reveal the other goat, if my initial choice was wrong, I would win by switching. Hence the probability of winning if I switch is $\frac{2}{3}$.
		
	<p>Since there are two opposing intuitions here, lets just do the math. For simplicity, let us assume you chose door number 1. We'll write $C_i$ for "the car is behind door i" and $M_i$ for "Monty picks door i". Initially the car is equally likely to be behind door 1, 2 or 3. If it's behind door 2 or 3, Monty's choice is determined, so $\Pr(M_3 \mid C_2) = \Pr(M_2 \mid C_3) = 1$. By contrast, $\Pr(M_2 \mid C_1) = \Pr(M_3 \mid C_1) = \frac{1}{2}$, assuming that Monty is equally likely to pick from each door. (Note that this is an implicit assumption not stated in the puzzle. If Monty always picks door 2 when available, these odds change, which makes for a nice exercise.) Applying Bayes' Theorem to the case where Monty picks door number 2:
		$$\Pr(C_1 \mid M_2) = \frac{\Pr(M_2 \cap C_1)\Pr(C_1)}{\Pr(M_2)} = \frac{1/2 \cdot 1/3}{1/2} = \frac{1}{3}$$
		whereas
		$$\Pr(C_3 \mid M_2) = \frac{\Pr(M_2 \cap C_3)\Pr(C_3)}{\Pr(M_2)} = \frac{1 \cdot 1/3}{1/2} = \frac{2}{3}$$
	So it indeed makes sense to switch to door 3. 	

	<!-- If we treat probability in the <em>Bayesian</em> sense, as a measure of uncertainty, it's reasonable to say that $\Pr(M_2 \mid C_1) = \frac{1}{2}$ regardless of whether it's specified in the puzzle, because both algorithms are equally likely.  -->
	
	<p>...assuming you want the car.

	<p><a href="https://xkcd.com/1282/"><img src="https://imgs.xkcd.com/comics/monty_hall.png" title="A few minutes later, the goat from behind door C drives away in the car." alt="Monty Hall" srcset="https://imgs.xkcd.com/comics/monty_hall_2x.png 2x" class="center"></a>
		
	<!-- <img src="https://imgs.xkcd.com/comics/monty_hall.png" alt=> -->
			
    <!-- <p><strong>Example 20.13.</strong> Consider again our biased die with $\Pr(1) = \frac 3 4$ and $\Pr(\omega) = \frac1{20}$ for $\omega = 2,3,4,5,6$. Suppose you go to a dice thrift store to pick up some second-hand dice. Unfortunately, there is a tendency for about 5% of the dice available to be biased in the way we described above and the remaining 95% of the dice are regular old fair dice. Because of this, the store will let you test-roll some dice. If you pick a die at random and roll two 1s, what is the probability that you picked up a biased die?

    <p>What we essentially want to do is compute the probability that, given that we rolled two 1s, we picked up a biased die. If we let $B$ denote the event that we picked up a biased die and we denote by $R_{1,1}$ the event that we rolled two 1s, then we want to compute $\Pr(B \mid R_{1,1})$. Bayes' Theorem tells us that
    $$\Pr(B \mid R_{1,1}) = \frac{\Pr(R_{1,1}) \mid B) \cdot \Pr(B)}{\Pr(R_{1,1})}.$$
    We know $\Pr(B) = 5\% = \frac 1 {20}$ and we know that $\Pr(R_{1,1} \mid B) = \left( \frac 3 4 \right)^2 = \frac{9}{16}$. All we need to do is to calculate $\Pr(R_{1,1})$.

    <p>Recall that the Law of Total Probability tells us that
    $$\Pr(R_{1,1}) = \Pr(R_{1,1} \mid B) \cdot \Pr(B) + \Pr(R_{1,1} \mid \overline B) \cdot \Pr(\overline B)$$
    where $\overline B$ is the non-biased die event, or in other words, the fair dice. This gives us
    \begin{align*}
    \Pr(R_{1,1}) &= \Pr(R_{1,1} \mid B) \cdot \Pr(B) + \Pr(R_{1,1} \mid \overline B) \cdot \Pr(\overline B) \\
    &= \frac{9}{16} \cdot \frac 1{20} + \frac 1{36} \cdot \frac{19}{20} \\
    &= \frac{157}{2880}
    \end{align*}
    Then plugging these into the formula for Bayes' Theorem gives us
    $$\Pr(B \mid R_{1,1}) = \frac{\frac{9}{320}}{\frac{157}{2880}} = \frac{81}{157} \approx 0.516.$$ -->

    <h2>Conditional probability and independence</h2>

    <p><strong>Definition 19.6.</strong> Events $A$ and $B$ are <strong>independent</strong> if $\Pr(A \cap B) = \Pr(A) \cdot \Pr(B)$.
 	  
    <p>This directly implies that if $A$ and $B$ are independent, then $\Pr(A \mid B) = \frac{\Pr(A) \cdot \Pr(B)}{\Pr(B)} = \Pr(A)$, which should fit our intuitions about conditional probability.
				
    <p><strong>Example 19.7.</strong> Consider a roll of a two six-sided dice and the events
	    <ul>
	        <li>$F_i$: the first roll comes up $i$,
	        <li>$S_j$: the second roll comes up $j$,
	        <li>$T_k$: the sum (or total) of the two rolls is $k$.
	    </ul>

		<p>Clearly, all the $F_i$s and $S_j$ are independent: The second roll doesn't depend on the outcome of the first, or vice-versa. Mathematically, for any $i$ and $j$, $$\Pr(F_i \cap S_j) = \frac{1}{36} = \frac{1}{6} \cdot \frac{1}{6} = \Pr(F_i) \cdot \Pr(S_j)$$

			Equally clearly, most $T_k$ <em>are</em> correlated with the $F_i$s: You can't get a total of 11 unless the first die comes up $5$ or $6$. The one exception is $T_7$: $\Pr(T_7 \mid F_i) = \frac{1}{6}$ and likewise for $S_i$. However, obviously $T_7$ isn't independent of the combination of $F_i$ and $S_i$s: $\Pr(T_7 \mid (F_4 \cap S_3)) = 1$ whereas $\Pr(T_7 \mid (F_4 \cap S_4)) = 0$. (We tend to write this as $\Pr(T_7 \mid F_4, S_4) = 0$.) This leads to want to distinguish between the independence of two events in isolation versus a larger group of events.
		
	    <p><strong>Definition 19.8.</strong> Events $A_1, A_2, \dots, A_k$ are <strong>pairwise independent</strong> if for all $i \neq j$, $A_i$ and $A_j$ are independent. The events are <strong>mutually independent</strong> if for every $I \subseteq \{1, \dots, k\}$,
	    $$\Pr \left( \bigcap_{i \in I} A_i \right) = \prod_{i \in I} \Pr(A_i).$$

    <p><strong>Definition 19.9.</strong> We say events $A$ and $B$ are <strong>positively correlated</strong> if $\Pr(A \cap B) \gt \Pr(A) \cdot \Pr(B)$ and they are <strong>negatively correlated</strong> if $\Pr(A \cap B) \lt \Pr(A) \cdot \Pr(B)$.
		
    <p><strong>Example 19.10.</strong> Consider a roll of a six-sided die and the events
    <ul>
        <li>$A$: the roll is odd,
        <li>$B$: the roll is prime.
    </ul>
    <p>Are these events independent? If not, how are they related? First, our sample space is $\Omega = \{1,2,3,4,5,6\}$, so our events are going to be $A = \{1,3,5\}$ and $B = \{2,3,5\}$. Then $\Pr(A) = \frac 3 6 = \frac 1 2$ and $\Pr(B) = \frac 3 6 = \frac 1 2$. We want to consider the probability of the intersection of our events. This gives us
    $$\Pr(A \cap B) = \Pr(\{1,3,5\} \cap \{2,3,5\}) = \Pr(\{3,5\}) = \frac 2 6 = \frac 1 3.$$
    But $\Pr(A) \cdot \Pr(B) = \frac 1 2 \cdot \frac 1 2 = \frac 1 4$, which is not $\frac 1 3$, so $A$ and $B$ are not independent. Furthermore, we have
    $$\Pr(A \cap B) = \frac 1 3 \gt \frac 1 4 = \Pr(A) \cdot \Pr(B),$$
    so $A$ and $B$ are positively correlated. Of course, this makes sense intuitively, because we know the only even prime number is 2.



    <!-- <p><strong>Example 19.10.</strong> Returning to the Monty Hall problem, we can use Bayes' theorem to formally describe the probability of the outcomes for each choice. Let $C$ be the random variable that indicates which door has the car behind it and let $D$ be the random variable that tells us which door Monty Hall opens. We denote each door by 1, 2, or 3. Suppose we choose door $i$.

    <p>First, we observe that $\Pr(C = j) = \frac 1 3$ for $j = 1,2,3$. That is, each door is equally likely to have the car behind it.

    <p>Next, we need to think about which door Monty is likely to open. Recalling that we chose door $i$, we have
    $$
    \Pr(D = k \mid C = j) =
        \begin{cases}
        1 & \text{if $i \neq j \neq k$,} \\
        0 & \text{if $k = i$ or $k = j$,} \\
        \frac 1 2 & \text{if $i = j$ and $k \neq i$.} \\
        \end{cases}
    $$
    The first case says that if we didn't choose the door with the car, then Monty must open the remaining door with the goat. The second case says that Monty will not open the door we chose or the door containing the car. The last case occurs when we choose the door with the car, in which case Monty opens one of the other two doors with equal probability.

    <p>Putting this together, we can consider two cases: if we chose the correct door ($C = i$) or if we chose the wrong door ($C = j \neq i$). In each case, Monty opens a door $k$. We know from the rules that $k \neq i$, since we chose $i$ and $k$ cannot be the door with the car behind it.

    <p>If we chose the correct door, we have
    $$\Pr(C = i \mid D = k) = \frac{\Pr(D = k \mid C = i) \Pr(C = i)}{\Pr(D = k)}.$$
    We must compute $\Pr(D = k)$, but that's not hard because we've exhausted all the possibilities above. We have
    \begin{align*}
    \Pr(D = k) &= \sum_{n=1}^3 \Pr(D = k \mid C = n) \Pr(C = n) \\
    &= 1 \cdot \frac 1 3 + 0 \cdot \frac 1 3 + \frac 1 2 \cdot \frac 1 3 \\
    &= \frac 1 2
    \end{align*}
    Then going back to our equation above, we have
    $$\Pr(C = i \mid D = k) = \frac{\frac 1 2 \cdot \frac 1 3}{\frac 1 2} = \frac 1 3.$$
    That is, if we picked the correct door to begin with, then Monty revealing the door doesn't change our odds of winning and we're still at $\frac 1 3$. Now, if we repeat this analysis but with $C = j \neq i$, we have
    $$\Pr(C = j \mid D = k) = \frac{1 \cdot \frac 1 3}{\frac 1 2} = \frac 2 3.$$
    This says that if we picked door $i$, then the probability that the car is behind the other door after Monty makes the revelation is $\frac 2 3$. Therefore, the best strategy is always to switch. -->
